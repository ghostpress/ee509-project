---
title: "EE 509 Project: Supplementary Models"
author: "Lucia Vilallonga"
output: html_document
---

Supplementary models: Bayesian multivariate linear regression shows that none of the covariates except x3 have an effect on Y, which is expected given the pairs plot.

# Setup
## Libraries
```{r, echo=FALSE}
library(sf)             ## GIS data
library(rjags)          ## MCMC
library(LaplacesDemon)  ## WAIC
library(MCMCvis)        ## Visualize MCMC outputs for particular params
library(pdftools)       ## convert MCMCvis PDF output to image
library(imager)         ## show images from file
```

## Load and Format Data
```{r}
X <- st_read("/home/lucia/bu/year4/semester2/EE509/project/ee509-project/data/ready/X/X.shp")
y <- read.csv("/home/lucia/bu/year4/semester2/EE509/project/ee509-project/data/ready/y/y.csv")

# Join the datasets along the town axis
PV <- merge(X, y, by="TOWN")

# Format data for MCMC
# First models will not include spatial variables
# For now, just use the average income and education values (not sum/highest) and %Biden as covariates
dat <- list(y=PV$totalInst, x1=PV$avgEd, x2=PV$avgInc, x3=PV$Biden.)
```

# Model Sa (supplementary): Bayesian multivariate linear regression

Process model:
$$\mu_i = \beta X_i$$
Data model:
$$y_i \sim Normal(\mu_i, S)$$
Parameter model:
$$\beta \sim Normal(B_0, V_b) \\ S \sim Gamma(s_1, s_2)$$
## Specify Model
```{r}
linear_glm_mv <- "
model {
  betas ~ dmnorm(B0, Vb)  ## prior on betas
  S ~ dgamma(s1, s2)      ## prior on precision
  
  for(i in 1:n) {
    mu[i] <- X[i,] %*% betas
    y[i] ~ dnorm(mu[i], S)
    like[i] <- dnorm(y[i], mu[i], S)
  }
}"
```

## MCMC Setup
```{r}
X <- model.matrix(~x1 + x2 + x3, data=dat)  ## prepends a column of 1s and puts xj into matrix X
datam <- list(y=dat$y, X=X)

# Specify priors
datam$B0 <- as.vector(c(0, 0, 0, 0))  ## priors on param.s means
datam$Vb <- solve(diag(10000, 4))     ## priors on param.s variances
datam$s1 <- 0.001
datam$s2 <- 0.001
datam$n  <- length(datam$y) ## n = no. of observations; m = no. of covariates

# Initialize JAGS
j.lin_mv <- jags.model(file=textConnection(linear_glm_mv),
                       data=datam, 
                       n.chains=3)
```

## Run MCMC
```{r}
# Run JAGS
jags.lin_mv <- coda.samples(model=j.lin_mv, 
                          variable.names=c("betas", "S", "like"),
                          n.iter=10000)

#out_lin_mv <- as.matrix(jags.lin_mv)
```

## Diagnostics
```{r}
# Split MCMC output
lin_mv_split <- codaSplit(jags.lin_mv, "like")
plot(lin_mv_split[[2]])

# Remove burn-in
jags.burn_lin_mv <- window(jags.lin_mv, start=500)
out_burn_lin_mv  <- as.matrix(jags.burn_lin_mv) 
jags.burn_lin_mv_split <- codaSplit(jags.burn_lin_mv, "like")

# Checking convergence & posterior densities
plot(jags.burn_lin_mv_split[[2]])

# Brooks-Gelman-Rubin
GBR <- gelman.plot(lin_mv_split[[2]])

# Effective sample size
effectiveSize(lin_mv_split[[2]])
```

## Density plots
```{r}
out_lin_mv_split <- as.matrix(lin_mv_split[[2]])

plot(density(out_lin_mv_split[,1]), main="Intercept", 
     sub="Density of MCMC samples for intercept term", cex.sub=0.8) 
plot(density(out_lin_mv_split[,2]), main="Beta[1]", 
     sub="Density of MCMC samples for slope on avgEd term", cex.sub=0.8)  
plot(density(out_lin_mv_split[,3]), main="Beta[2]", 
     sub="Density of MCMC samples for slope on avgInc", cex.sub=0.8) 
plot(density(out_lin_mv_split[,4]), main="Beta[3]", 
     sub="Density of MCMC samples for slope on %Biden", cex.sub=0.8)  
```

## Model Fit
```{r}
# DIC
dic.lin_mv <- dic.samples(j.lin_mv, 1000, "pD")
dic.lin_mv <- sum(dic.lin_mv$deviance) + sum(dic.lin_mv$penalty)

# WAIC
waic.lin_mv <- WAIC(as.matrix(lin_mv_split[[1]]))

print(paste("DIC: ", round(dic.lin_mv, digits=3), " WAIC: ", round(waic.lin_mv$WAIC, digits=3)))
```

## Parameter Confidence Intervals
```{r}
cS.lin_mv <- quantile(out_lin_mv_split[,"S"], c(0.025, 0.975))         ## S 
c1.lin_mv <- quantile(out_lin_mv_split[,"betas[1]"], c(0.025, 0.975))  ## beta[1] (intercept)
c2.lin_mv <- quantile(out_lin_mv_split[,"betas[2]"], c(0.025, 0.975))  ## beta[2] (slope on x1, avgEd)
c3.lin_mv <- quantile(out_lin_mv_split[,"betas[3]"], c(0.025, 0.975))  ## beta[3] (slope on x2, avgInc)
c4.lin_mv <- quantile(out_lin_mv_split[,"betas[4]"], c(0.025, 0.975))  ## beta[4] (slope on x3, %voBiden)
```

# Model Sb (supplementary): Bayesian multivariate linear regression with errors in income data

Process model:
$$\mu_i = \beta X_i$$

General errors in variables data model:
$$y_i \sim Normal(\mu_i, S) \\ X_{inc, i, observed} \sim Normal(X_{inc, i}, \alpha_i X_{inc, i})$$

Parameter model:
$$\beta \sim Normal(B_0, V_b) \\ S \sim Gamma(s_1, s_2) \\ \alpha \sim Normal(a_0, V_a)$$

In this case, the American Community Survey (ACS) income data also reports Margins of Error (MOE) for each estimate, which can be fed directly into JAGS as precisions, instead of estimating with the alpha parameters:

## SE Calculations for errors in income data
```{r}
MOE <- PV$moeInc
SE  <- abs(MOE) / 1.645   ## source: US Census Bureau, 2009
```

## Specify Model 
```{r}
linear_glm_he <- "
model {
  betas ~ dmnorm(B0, Vb)  ## prior on betas
  S ~ dgamma(s1, s2)
  
  for(i in 1:n) {
    X[i,3] ~ dnorm(dummy_x3[i], SE[i])  ## income data is in the 3rd column of X
    x3.hat[i] <- X[i,3]
  }
  
  for(i in 1:n) {
    mu[i] <- X[i,] %*% betas
    
    y[i] ~ dnorm(mu[i], S)
    like[i] <- dnorm(y[i], mu[i], S)
  }
}"
```

## MCMC Setup
```{r}
datah <- list(y=dat$y, X=X, SE=SE)

# Create a dummy vector x3 for the income data:
datah$dummy_x3 <- datah$X[,3]

# Specify priors:
datah$B0 <- as.vector(c(0, 0, 0, 0))
datah$Vb <- solve(diag(10000, 4))
datah$s1 <- 0.001
datah$s2 <- 0.001
datah$n  <- length(datah$y)

# Initialize JAGS:
j.lin_he <- jags.model(file=textConnection(linear_glm_he),
                       data=datah,
                       n.chains=3)
```

```{r}
jags.lin_he <- coda.samples(model=j.lin_he,
                            variable.names=c("betas", "S", "x3.hat", "like"),
                            n.iter=10000)
```

```{r}
out_he <- as.matrix(jags.lin_he)

plot(density(datah$dummy_x3), main="Observed incomes")
plot(density(out_he[,356:705]), main="Sampled incomes using MOE")
```

## Diagnostics
```{r}
# Plots are commented out below bc couldn't split MCMC object 2x, 
# so with like and x3.hat there are 705 plots total
# plot(jags.lin_he)  ## good convergence on betas, S
# GBR <- gelman.plot(jags.lin_he)  ## burn-in should be around 1000

# Burn-in:
jags.burn_lin_he <- window(jags.lin_he, start=1000)

# Using MCMCvis to get the trace plots of just S and betas
MCMCtrace(jags.burn_lin_he, 
          params=c("S", "betas[1]", "betas[2]", "betas[3]", "betas[4]"),
          ISB=FALSE,
          exact=TRUE)  ## outputs a PDF file in working directory

# Show trace plots:
pdf_convert("MCMCtrace.pdf", format="png")  ## convert to png file
knitr::include_graphics("/home/lucia/bu/year4/semester2/EE509/project/ee509-project/code/modeling/MCMCtrace_1.png")
knitr::include_graphics("/home/lucia/bu/year4/semester2/EE509/project/ee509-project/code/modeling/MCMCtrace_2.png")

#effectiveSize(jags.burn_lin_he)  ## >10,000 for S, betas
```

## Model Fit
```{r}
# DIC
dic.lin_he <- dic.samples(j.lin_he, 1000, "pD")
dic.lin_he <- sum(dic.lin_he$deviance) + sum(dic.lin_he$penalty)

# WAIC
lin_he_split <- codaSplit(jags.lin_he, "like")
waic.lin_he <- WAIC(as.matrix(lin_he_split[[1]]))

print(paste("DIC: ", round(dic.lin_he, digits=3), " WAIC: ", round(waic.lin_he$WAIC, digits=3)))
```

## Parameter Confidence Intervals
```{r}
out_burn_he <- as.matrix(jags.burn_lin_he)

cS.lin_he <- quantile(out_burn_he[,"S"], c(0.025, 0.975))         ## S
c1.lin_he <- quantile(out_burn_he[,"betas[1]"], c(0.025, 0.975))  ## beta[1] (intercept)
c2.lin_he <- quantile(out_burn_he[,"betas[2]"], c(0.025, 0.975))  ## beta[2] (slope on x1, avgEd)
c3.lin_he <- quantile(out_burn_he[,"betas[3]"], c(0.025, 0.975))  ## beta[3] (slope on x2, avgInc)
c4.lin_he <- quantile(out_burn_he[,"betas[4]"], c(0.025, 0.975))  ## beta[4] (slope on x3, %Biden)
```


