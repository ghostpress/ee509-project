---
title: "EE 509 Project: Preliminary Analysis"
author: "Lucia Vilallonga"
output: html_notebook
---

# Background

3 models to fit: (1) Bayesian linear regression; (2) Bayesian linear regression with heteroskedasticity in income data; and (3) time-series analysis. Objective: describe and determine factors for adoption of residential rooftop solar systems in Massachusetts towns.

# Setup
## Libraries
```{r, echo=FALSE}
library(sf)             ## GIS data
#library(imager)        ## loading images
library(rjags)          ## MCMC
library(LaplacesDemon)  ## WAIC
```

## Load and Format Data
```{r}
X <- st_read("/home/lucia/bu/year4/semester2/EE509/project/ee509-project/data/ready/X/X.shp")
y <- read.csv("/home/lucia/bu/year4/semester2/EE509/project/ee509-project/data/ready/y/y.csv")

# Join the datasets along the town axis
PV <- merge(X, y, by="TOWN")

# Format data for MCMC
# First model will not include spatial variables
# For now, just use the average income and education values (not sum/highest)
dat <- list(y=PV$totalInst, x1=PV$avgEd, x2=PV$avgInc, x3=PV$Biden.)
X <- model.matrix(~x1 + x2 + x3, data=dat)  ## prepends a column of 1s and puts xj into matrix X
```

# First model: Bayesian GLM with Poisson regression

Process model:
$$log(\lambda_i) = \boldsymbol{\beta} X_i$$
Data model:
$$y_i \sim Pois(\lambda_i)$$
Parameter model:
$$\beta \sim Normal(B_0, V_b)$$
Log-likelihood:
$$ln(L) = \sum_{i=1}^n y_i(\beta X_i) - \sum_{i=1}^ne^{\beta X_i}$$

## Specify Model
```{r}
poisson_glm <- "
model {
  beta ~ dmnorm(B0, Vb)  ## prior on betas

  for(i in 1:n) {
    log(lambda[i]) <- beta[1] + beta[2]*x[i]    ## process model
    y[i] ~ dpois(lambda[i])                     ## data model 
    like[i] <- y[i]*log(lambda[i]) - lambda[i]  ## log-likelihoods
  }
}"
```

Poisson regression doesn't seem to work; can't get the model to converge, even with upwards of 200,000 samples and playing around with initial values. The effective sample size is always in the single digits. Moving on to try a Bayesian linear model instead.

# Model 1a: Bayesian linear regression

Process model:
$$\mu_i = \beta X_i$$
Data model:
$$y_i \sim Normal(\mu_i, S)$$
Parameter model:
$$\beta \sim Normal(B_0, V_b) \\ S \sim Gamma(s_1, s_2)$$
## Specify Model
```{r}
linear_glm_mv <- "
model {
  betas ~ dmnorm(B0, Vb)  ## prior on betas
  S ~ dgamma(s1, s2)      ## prior on precision
  
  for(i in 1:n) {
    mu[i] <- X[i,] %*% betas
    y[i] ~ dnorm(mu[i], S)
    like[i] <- dnorm(y[i], mu[i], S)
  }
}"
```

## MCMC Setup
```{r}
datam <- list(y=dat$y, X=X)

# Specify priors
datam$B0 <- as.vector(c(0, 0, 0, 0))  ## priors on param.s means
datam$Vb <- solve(diag(10000, 4))     ## priors on param.s variances
datam$s1  <- 0.001
datam$s2  <- 0.001
datam$n  <- length(datam$y) ## n = no. of observations; m = no. of covariates

# Initialize JAGS
j.lin_mv <- jags.model(file=textConnection(linear_glm_mv),
                       data=datam, 
                       n.chains=3)
```

## Run MCMC
```{r}
# Run JAGS
jags.lin_mv <- coda.samples(model=j.lin_mv, 
                          variable.names=c("betas", "S", "like"),
                          n.iter=10000)

#out_lin_mv <- as.matrix(jags.lin_mv)
```

## Splitting JAGS output
Code source: Michael Dietze
```{r}
# Function to split JAGS output
codaSplit <- function(jags.out,pattern){
  out = list()
  mfit = as.matrix(jags.out,chains=TRUE)
  pat.cols = grep(pattern,colnames(mfit),fixed=TRUE)
  chain.col = which(colnames(mfit)=="CHAIN")
  out[[1]] = mat2mcmc.list(mfit[,c(chain.col,pat.cols)])
  out[[2]]   = mat2mcmc.list(mfit[,-pat.cols])
  return(out)
}

mat2mcmc.list <- function(w) {
  temp <- list()
  chain.col <- which(colnames(w) == "CHAIN")
  for (i in unique(w[, "CHAIN"])) {
    temp[[i]] <- coda:::as.mcmc(w[w[, "CHAIN"] == i, -chain.col])
  }
  return(as.mcmc.list(temp))
}
```

## Diagnostics
```{r}
# Split MCMC output
lin_mv_split <- codaSplit(jags.lin_mv, "like")
plot(lin_mv_split[[2]])

# Remove burn-in
jags.burn_lin_mv <- window(jags.lin_mv, start=500)
jags.burn_lin_mv_split <- codaSplit(jags.burn_lin_mv, "like")

# Checking convergence & posterior densities
plot(jags.burn_lin_mv_split[[2]])

# Brooks-Gelman-Rubin
GBR <- gelman.plot(lin_mv_split[[2]])

# Effective sample size
effectiveSize(lin_mv_split[[2]])
```

## Density plots
```{r}
out_lin_mv_split <- as.matrix(lin_mv_split[[2]])

plot(density(out_lin_mv_split[,1]), main="Intercept", 
     sub="Density of MCMC samples for intercept term", cex.sub=0.8) 
plot(density(out_lin_mv_split[,2]), main="Beta[1]", 
     sub="Density of MCMC samples for slope on avgEd term", cex.sub=0.8)  
plot(density(out_lin_mv_split[,3]), main="Beta[2]", 
     sub="Density of MCMC samples for slope on avgInc", cex.sub=0.8) 
plot(density(out_lin_mv_split[,4]), main="Beta[3]", 
     sub="Density of MCMC samples for slope on %Biden", cex.sub=0.8)  
```
## Model Fit
```{r}
# DIC
dic.lin_mv <- dic.samples(j.lin_mv, 1000, "pD")
dic.lin_mv <- sum(dic.lin_mv$deviance) + sum(dic.lin_mv$penalty)

# WAIC
waic.lin_mv <- WAIC(as.matrix(lin_mv_split[[1]]))

print(paste("DIC: ", round(dic.lin_mv, digits=3), " WAIC: ", round(waic.lin_mv$WAIC, digits=3)))
```

## Parameter Confidence Intervals
```{r}
c1.lin_mv <- quantile(out_lin_mv_split[,"betas[1]"], c(0.025, 0.975))  ## beta[1] (intercept)
c2.lin_mv <- quantile(out_lin_mv_split[,"betas[2]"], c(0.025, 0.975))  ## beta[2] (slope on x1, avgEd)
c3.lin_mv <- quantile(out_lin_mv_split[,"betas[3]"], c(0.025, 0.975))  ## beta[3] (slope on x2, avgInc)
c4.lin_mv <- quantile(out_lin_mv_split[,"betas[4]"], c(0.025, 0.975))  ## beta[4] (slope on x3, %voBiden)
```

# Second Model: Bayesian linear regression with heteroskedasticity in income data

Process model:
$$\mu_i = \beta X_i$$

Data model:
$$y_i \sim Normal(\mu_i, S) \\ X_{inc, i, observed} \sim Normal(X_{inc, i}, \alpha_i X_{inc, i})$$

Parameter model:
$$\beta \sim Normal(B_0, V_b) \\ S \sim Gamma(s_1, s_2) \\ \alpha \sim Normal(a_0, V_a)$$
## SE Calculations for heteroskedastic income data
```{r}
MOE <- PV$moeInc
SE  <- abs(MOE) / 1.645   ## source: US Census Bureau, 2009
```

## Specify Model 
```{r}
linear_glm_he <- "
model {
  betas ~ dmnorm(B0, Vb)  ## prior on betas
  #alpha ~ dlorm(a1, Va)   ## prior on X variance param. (zero-bound)
  S ~ dgamma(s1, s2)
  
  for(i in 1:n) {
    X[i,3] ~ dnorm(dummy_x3[i], SE[i])  ## income data is in the 3rd column of X
    x3.hat[i] <- X[i,3]
  }
  
  for(i in 1:n) {
    mu[i] <- X[i,] %*% betas
    
    #s[i] <- alpha*X[i,3]  
    #S[i] <- 1/s[i]^2
    
    y[i] ~ dnorm(mu[i], S)
    like[i] <- dnorm(y[i], mu[i], S)
  }
}"
```

## MCMC Setup
```{r}
datah <- list(y=dat$y, X=X, SE=SE)

# Create a dummy vector x3 for the income data:
datah$dummy_x3 <- datah$X[,3]

# Specify priors:
datah$B0 <- as.vector(c(0, 0, 0, 0))
datah$Vb <- solve(diag(10000, 4))
datah$s1 <- 0.001
datah$s2 <- 0.001
datah$n  <- length(datah$y)

# Initialize JAGS:
j.lin_he <- jags.model(file=textConnection(linear_glm_he),
                       data=datah,
                       n.chains=3)
```

```{r}
jags.lin_he <- coda.samples(model=j.lin_he,
                            variable.names=c("betas", "S", "x3.hat", "like"),
                            n.iter=10000)
```

```{r}
out_he <- as.matrix(jags.lin_he)

plot(density(datah$dummy_x3), main="Reported incomes")
plot(density(out_he[,356:705]), main="Sampled incomes using MOE")
```

TODO: 
1. CIs for lin_mv model
2. model 2: heteroskedasticity
3. model 3: time-series